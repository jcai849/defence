<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>A Platform for Large Scale Statistical Modelling Using R</title>
    <style>
      .img-row {
    display: flex;
    justify-content: center;
    flex-wrap: wrap;
    gap: 2em;
  }
  .img-row img {
    max-width: 30%;
    height: auto;
  }
    </style>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section data-markdown class="center">
          <textarea data-template>
            ## A Platform for Large Scale Statistical Modelling Using R

            Jason Cairns

            Supervised by: Simon Urbanek, Paul Murrell
          </textarea>
        </section>
        <section class="center">
          <section class="center"><h2>Introduction</h2></section>
          <section data-markdown class="center">
            <textarea data-template>
              ## Problem

              - Larger than memory datasets
              - Want to express and perform complex statistical algorithms in R over them
            </textarea>
          </section>
          <section data-markdown class="center">
            <textarea data-template>
              ## My Claim

              - A sound interface is required, in order to guide thought for expressing distributed computation
              - The interface just needs to be discovered, and the implementation needs to follow
            </textarea>
          </section>
        </section>
        <section class="center">
          <section class="center"><h2>Key Concept: Large Scale</h2></section>
          <section class="center"><ul>
              <li class="fragment fade-in-then-semi-out">Splitting into chunks: essential concept</li>
              <li class="fragment fade-in-then-semi-out">Multiple machines for scalability (vs. concurrent approach)</li>
              <li class="fragment fade-in-then-semi-out">Communication: topology, efficiency</li>
            </ul></section>
        </section>
        <section class="center">
          <section class="center"><h2>Prior Art</h2></section>
          <section class="center">
            <h2>Platforms</h2>
            <img
              src="https://upload.wikimedia.org/wikipedia/commons/3/38/Hadoop_logo_new.svg"
              width="30%"
              height="30%"
            >
            <img src="https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg" width="30%" height="30%">
            <aside class="notes">
              Hadoop: Collection of utilities, around a distributed memory part
                      (HDFS), and processing, MapReduce. Pioneering, but limited
                      for iterative computation
              Spark: Builds upon hadoop, allowing for iteration over a Resilient
                     Distributed Dataset, and increasingly, off-the-shelf ML
                     operations, but inexpressive for novel statistical algorithms
            </aside>
          </section>
          <section class="center">
            <h2><img src="https://upload.wikimedia.org/wikipedia/commons/1/1b/R_logo.svg" style="height: 1em; vertical-align: middle;">
              Packages</h2>
            <img src="https://spark.posit.co/images/favicon/apple-touch-icon.png" width="30%" height="30%">
            <img src="https://pbdr.org/ui/img/newpbdr.png" width="30%" height="30%">
            <img src="https://cran.r-project.org/web/packages/disk.frame/readme/inst/figures/disk.frame.png" width="30%" height="30%">
            <p><b>SNOW</b>
              (parallel)</p>
            <aside class="notes">
              sparklyr: An interface to spark from R, still off-the-shelf, little primitives
            pbdr: Almost ideal. For MPI-based computation, requires SIMD
                  programming (non-interactive). Other computations allow
                  for interactivity, but only offer RPC tooling and little
                  more. Distributed matrices, but no other arbitrary data,
                  such as distributed data frames
            disk.frame: actually deprecated in favour of arror since
                        thesis submission. Arbitrarily large datasets on disk,
                        read streamwise. Inherently non-parallel.
            SNOW: Extremely simple setup and teardown of cluster, can push
                  computations, but results don't persist in a straightforward
                  manner on the worker nodes
            </aside>
          </section>
          <section class="center">
            <div style="transform: scale(0.35); transform-origin: top;">
              <div id="comparison-table"></div>
            </div>
          </section>
        </section>
        <section class="center">
          <section class="center"><h2>User Interface</h2></section>
          <section data-markdown class="center">
            <textarea data-template>
              ## Layered Approach

              - Invariant to distribution of data structures (space / time)
              - Arbitrary underlying data
              - Model Usage, Description and Low-level interaction
            </textarea>
          </section>
          <section data-markdown class="center">
            <textarea data-template>
              ## Model Usage
              ### Largescalemodels

              ```r
              x_hat <- dlasso(X,y)
              ```
            </textarea>
          </section>
          <section data-markdown class="center">
            <textarea data-template>
              ## Model Description
              ### largescaleobjects

              ```r
              b <- A %*% x + e
              ...
              shuffle(X, by=b)
              ```
            </textarea>
          </section>
          <section data-markdown class="center">
            <textarea data-template>
              ## Low-level Interaction
              ### chunknet, orcv

              ```r
              tab <- do.ccall(table, x)
              emerge(tab)
              ```
            </textarea>
          </section>
        </section>
        <section class="center">
          <section class="center"><h2>Design & Implementation</h2></section>
          <section class="center">
            <h2>Distributed Object</h2>
            <div id="distobjref" style="text-align:center;"></div>
            <aside class="notes">Array of Chunk References</aside>
          </section>
          <section class="center">
            <h2>Chunk Calling</h2>
          </section>
          <section class="center">
            <h2>Chunk Lineage</h2>
            <div id="gc" style="text-align:center;"></div>
          </section>
          <section class="center">
            <h3>Distributed Object Calling</h3>
            <ul>
              <li>e.g. biglm</li>
              <li>Simplification: One chunk at a time.</li>
              <li>Initial chunk call, followed by rolling update along chunks</li>
              </ul>
          </section>
          <section>
            <h3>Distributed Object Calling</h3>
            <div id="dlm" style="text-align:top;"></div>
          </section>
          <section class="center">
            <h2>Distributed LASSO</h2>
              \[
                \begin{aligned}
                  A = \begin{bmatrix}
                      A_1\\
                      \vdots \\
                      A_N
                  \end{bmatrix},&
                  \quad b=\begin{bmatrix}
                      b_1\\
                      \vdots \\
                      b_N
                  \end{bmatrix}\\
                  A_i \in \mathbb{R}^{m_i\times n},& \quad b_i \in \mathbb{R}^{m_i}
              \end{aligned}
            \]
          </section>
          <section class="center">
              \[
            \begin{aligned}
              x^{k+1} &= \arg \min_x \frac{1}{2} \left|\| Ax - b \right\|^2_2 + \frac{\rho}{2} \left\| x - z + u \right\|^2_2 \\
              z^{k+1} &= S_z(\overline{x}^{k+1} +\overline{u}) \\
              u^{k+1} &= u + x^{k+1} - z^{k+1}
            \end{aligned}
            \]
            <aside class="notes">
              S is soft thresholding parameter
              rho is penalty parameter
            </aside>
          </section>
          <section class="center">
              \[
              x^{k+1} = \arg \min_x \frac{1}{2} \left|\| Ax - b \right\|^2_2 + \frac{\rho}{2} \left\| x - z + u \right\|^2_2
            \]
            <pre><code data-trim class="language-r"><script type="text/template">
                d.x_update <- d(function(x_prev, A, b, u_prev, rho, z_prev) {
                  argmin(x_prev, function(x_prev)
                    (1/2)*l2_norm(A %*% x_prev - b)^2 +
                    (rho/2)*l2_norm(x_prev - z_prev + u_prev)^2)
                })
            </script></code></pre>
            <aside class="notes">
              S is soft thresholding parameter
              rho is penalty parameter
            </aside>
          </section>
          <section class="center">
              \[
            \begin{aligned}
              x^{k+1} &= \ldots \\
              z^{k+1} &= S_z(\overline{x}^{k+1} +\overline{u}) \\
              u^{k+1} &= u + x^{k+1} - z^{k+1}
            \end{aligned}
            \]
            <pre><code data-trim class="language-r"><script type="text/template">
                while (l1_norm(z_curr - z_prev) > tolerance) {
                  ...
                  x_curr <- d.x_update(x_prev, A, b, u_prev, rho, z_prev)
                  z_curr <- S_z(rowMeans(emerge(x_curr)) +
                                rowMeans(emerge(u_curr)))
                  u_curr <- u_prev + x_curr - z_curr
                }
            </script></code></pre>
            <aside class="notes">
              S is soft thresholding parameter
              rho is penalty parameter
            </aside>
          </section>
        </section>
        <section class="center">
          <section class="center"><h2>Live Demonstration</h2></section>
          <section class="center"><h2>Backup Demonstration</h2>
            <div id="lsmr" class="player"></div></section>
        </section>
        <section class="center"><section class="center"><h2>Discussion</h2></section>
          <section data-markdown class="center">
            <textarea data-template>
              ## Discussion

              - A further step
              - Simple by design
              - Extensions
                - Self-healing data
                - Refinement & robustness in face of real-world use
            </textarea>
          </section></section>
        <section class="center"><section class="center"><h2>Appendix</h2></section>
          <section class="center">
            <h2>orcv</h2>
            <img src="images/orcv.svg">
            <aside class="notes">
              A "Listener" for incoming connections is started in a
              separate thread. When it receives a connection request, it
              places the connected file descriptor and a reference to a
              "Background Queue" into a receiver queue.

              Listening to the "Receiver Queue" is a group of "Receivers",
              each on its own thread, whose job it is to read from the
              supplied file descriptor and place the read data into the
              provided queue.

              The calling R process can perform a blocking pop on this
              background queue.

              Alternatively, the R process may have an ongoing
              session with another process, maintaining the same file
              descriptor. In this case, a "File Descriptor Queue" is
              passed along with the file descriptor into the receiver
              queue. The receiver's behave invariantly, placing read
              data into the passed queues, which the R process performs
              a blocking pop on.
            </aside>
          </section>
          <section data-vertical-align-top>
            <h2>chunknet</h2>
            <div id="chunknet" style="text-align:top;"></div>
            <aside class="notes">
              To call computations on chunks in a distributed manner.
          
              A "Master" or "Client" will determine the locations that a
              computation will take place, based on a target location, if
              any, existing chunk locations which minimise data movement,
              and current loading of worker nodes.

              A "Locator" table is queried for non-cached locations and
              loading, in aid of this. Data is pushed to locations if needed,
              and the Locator table is  updated to reflect locations of newly
              pushed data. Currently this is done with a special "Locator"
              service on its own node, but could in theory be run over a
              Distributed Hash Table, for a fully peer-to-peer topology.

              The computations are then pushed to their locations.

              From the requesting client's perspective, the ID's and the
              computation references are created and composed within chunk
              references to the the computation output.

              The computation chunk references are recorded with their
              locations in the Locator table.

              You can see the recursive nature of Chunk References tracking
              their originating computations, and Computation References
              tracking their input Chunk References. This is the way Garbage
              Collection is performed; with no references remaining on the
              client, R performs its sweep, and the chunk reference finalisers
              issue a DELETE command on the whole chain.

              On the worker, there is a flurry of activity upon receiving
              a computation request.

              The argument chunks are all registered in its internal
              Data Store. Chunks that should exist on other nodes are
              asynchronously requested. The computation is registered in a
              Computation Store, indexed by its arguments. If all argument
              chunks are available locally, the computation is run and the
              main Event Loop is returned to.

              If data was pending externally, what happens in detail is that
              the locator table is queried for the locations of the chunks,
              and a request for the data is forwarded to those locations. At
              the requested locations, that node will assess its own local
              Data Store. If the requested data is there, it will immediately
              send it. If the data is the result of a computation on its
              own node, it will mark the chunk the the "Audience" location
              requesting it, in order to send it when ready.

              Once the data is send to a worker node, it will do a lookup
              of the data against its own Data Store, responding to any
              external nodes awaiting the data and also checking which locally
              pending computations require this data. If any compuations
              do, it will mark this prerequisite fulfilled.  If this is the
              last prerequisite for a computation, that computation is run,
              and any output of the computation is posted back to the noted
              itself, to trigger the process of audience response and pending
              computations again.

              Clients may chain do.ccalls without any intermediate
              computations required to complete; they are non-blocking. When
              output is firmly desired, a blocking request is made for the
              data of the final chunk reference, and this triggers that same
              data lookup on the location of the worker node, but with a file
              descriptor rather than a location being stored as the audience.
            </aside>
          </section>
          <section class="center">
            <img src="images/worker-data.svg">
            <img src="images/locator-data.svg">
          </section>
          <section data-markdown class="center">
            <textarea data-template>
              ## `do.dcall()`

              - Wrapper around `do.ccall()`
              - May align arguments beforehand (rabbithole)
              - Magic:
              - `d(f) == \(...) do.dcall(f, ...)`
            </textarea>
          </section>
          <section data-markdown class="center">
            <textarea data-template>
              ## `emerge()`

              - `pull()` then `combine()`; `combine` generic on args
              - e.g. `table()`
            </textarea>
          </section>
          <section>
            <h2>dReduce()</h2>
            <p>Rolling binary application of ccall()</p>
            <div id="dreduce" style="text-align:center;"></div>
          </section>
          <section data-markdown class="center">
            <textarea data-template>
              ## `map_reduce(map, reduce)`

              - `do.dcall(map, ...) |> emerge() |> reduce()`
              - e.g. `summary() == map_reduce(.Generic, .Generic)`
              - e.g. `sum() == do.dcall(sum) |> emerge() |> combine |> sum()` where `combine = identity`
            </textarea>
          </section>
          <section data-markdown class="center">
            <textarea data-template>
              ## `shuffle(X, index)`

              - as used in `group_by()`
              - tabulate index and find even distribution of keys based on index frequency.
              - for each key, subset each chunk for corresponding components.
              - let worker nodes shuffle themselves.
            </textarea>
          </section>
        </section>
      </div>
    </div>
    <script type="module" src="scripts/main.js"></script>
  </body>
</html>
